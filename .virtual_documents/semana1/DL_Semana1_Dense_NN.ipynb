





import tensorflow as tf
import numpy as np
import math

import matplotlib.pyplot as plt


# Example: a simple linear transformation
x = tf.constant([[1.0, 2.0]], dtype=tf.float32)  # input vector

#x = tf.Variable([[1.0, 2.0]], dtype=tf.float32)

#x = np.array([[1.0, 2.0]])
#x = tf.convert_to_tensor(x, dtype=tf.float32)

W = tf.constant([[2.0, -1.0],
                 [0.5, 3.0]], dtype=tf.float32)  # weight matrix
b = tf.constant([1.0, -2.0], dtype=tf.float32)   # bias

y = tf.matmul(x, W) + b
y


tf.keras.activations.sigmoid(tf.matmul(x, W) + b)





# Data
X = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)
y_true = np.sin(X)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='tanh', input_shape=(1,)),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='adam', loss='mse')
model.fit(X, y_true, epochs=500, verbose=1)

y_pred = model.predict(X)


plt.plot(X, y_true, label="sin(x)")
plt.plot(X, y_pred, label="NN approx")
plt.legend()
plt.show()


def taylor_sin(x, order):
    n_terms = (order // 2) + 1  
    out = np.zeros_like(x)
    for k in range(n_terms):
        p = 2*k + 1
        out = out + ((-1)**k) * np.power(x, p) / math.factorial(p)
    return out


taylor_preds = taylor_sin(X, 5)


plt.plot(X, y_true, label="sin(x)")
plt.plot(X, y_pred, label="NN approx")
plt.plot(X, taylor_preds, label="Taylor approx")
plt.legend()
plt.show()



def mse(a, b):
    return float(np.mean((a - b)**2))

def max_abs_err(a, b):
    return float(np.max(np.abs(a - b)))

nn_mse = mse(y_pred, y_true)
nn_max = max_abs_err(y_pred, y_true)

t_mse = mse(taylor_preds, y_true)
t_max = max_abs_err(taylor_preds, y_true)


metrics = {
    'NeuralNet': {'MSE': nn_mse, 'MaxAbsErr': nn_max},
    'Taylor': {'MSE': t_mse, 'MaxAbsErr': t_max},
}



print("{:<15s}  {:>12s}  {:>12s}".format("Approximator", "MSE", "Max|err|"))
for k, v in metrics.items():
    print("{:<15s}  {:>12.6e}  {:>12.6e}".format(k, v['MSE'], v['MaxAbsErr']))





def F(x):
    return (x + 1)**2


x = tf.Variable(initial_value=2.0, name='x', trainable=True, dtype=tf.float32)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)


for _ in range(1000):

    with tf.GradientTape() as tape:
        y = F(x) 
        
    gradients = tape.gradient(y, [x])
    optimizer.apply_gradients(zip(gradients, [x]))
    print("y:",y.numpy(), "x:",x.numpy())





# Data
X = np.linspace(-2*np.pi, 2*np.pi, 200).reshape(-1, 1)
y_true = np.sin(X)


class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.d1 = tf.keras.layers.Dense(258, activation='tanh')
        self.d2 = tf.keras.layers.Dense(258, activation='tanh')
        self.out = tf.keras.layers.Dense(1)

    def call(self, x):
        x = self.d1(x)
        x = self.d2(x)
        return self.out(x)

# Instantiate
model = MyModel()


# Optimizer and loss
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
loss_fn = tf.keras.losses.MeanSquaredError()

dataset = tf.data.Dataset.from_tensor_slices((X, y_true)).batch(32)

# Training step
@tf.function
def train_step(x_batch, y_batch):
    with tf.GradientTape() as tape:
        predictions = model(x_batch, training=True)
        loss = loss_fn(y_batch, predictions)
    # Compute gradients
    gradients = tape.gradient(loss, model.trainable_variables)
    # Apply updates
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss


EPOCHS = 100
for epoch in range(EPOCHS):
    for step, (x_batch, y_batch) in enumerate(dataset):
        loss = train_step(x_batch, y_batch)
    print(f"Epoch {epoch+1}, Loss: {loss.numpy():.4f}")


y_pred = model.predict(X)


plt.plot(X, y_true, label="sin(x)")
plt.plot(X, y_pred, label="NN approx")
plt.legend()
plt.show()





# Plot activations and derivatives
activations = {
    "sigmoid": tf.nn.sigmoid,
    "tanh": tf.nn.tanh,
    "relu": tf.nn.relu,
    "gelu": tf.nn.gelu
}

x = np.linspace(-5, 5, 200)

plt.figure(figsize=(12,5))
for i, (name, f) in enumerate(activations.items()):
    plt.subplot(1,4,i+1)
    y = f(x).numpy()
    plt.plot(x, y, label=name)
    plt.title(name)
plt.show()











# Datos sintéticos
X = np.linspace(-3, 3, 200).reshape(-1, 1)
y_true = 0.5 * X + 1 + 0.3 * np.random.randn(*X.shape)

# Añadimos outliers
y_true[::20] += np.random.randn(*y_true[::20].shape) * 6

plt.scatter(X, y_true, alpha=0.6)
plt.title("Datos sintéticos con outliers")
plt.show()


def build_reg_model():
    return tf.keras.Sequential([
        tf.keras.layers.Dense(32, activation="relu", input_shape=(1,)),
        tf.keras.layers.Dense(1)
    ])

losses = {
    "MSE": tf.keras.losses.MeanSquaredError(),
    "MAE": tf.keras.losses.MeanAbsoluteError(),
    "Huber": tf.keras.losses.Huber(delta=1.0)
}

histories = {}
preds = {}

for name, loss_fn in losses.items():
    model = build_reg_model()
    model.compile(optimizer="adam", loss=loss_fn)
    h = model.fit(X, y_true, epochs=100, verbose=0)
    histories[name] = h.history['loss']
    preds[name] = model.predict(X)

for name, loss in histories.items():
    plt.plot(loss, label=name)
plt.legend()
plt.title("Comparación de pérdidas en regresión con outliers")
plt.show()


plt.scatter(X, y_true, alpha=0.6)
plt.plot(X, preds['MSE'], label="MSE")
plt.plot(X, preds['MAE'], label="MAE")
plt.plot(X, preds['Huber'], label="Huber")
plt.title("Datos sintéticos con outliers")
plt.legend()
plt.show()








# Supongamos 3 clases
y_true_onehot = np.array([[0, 0, 1],   # clase 2
                          [1, 0, 0]])  # clase 0

y_true_onehot


y_true_sparse = np.array([2, 0])       # enteros: clase 2 y clase 0


# Predicciones (softmax simuladas)
y_pred = np.array([[0.1, 0.2, 0.7], 
                   [0.8, 0.1, 0.1]])


# Usando categorical_crossentropy (one-hot)
loss_cat = tf.keras.losses.CategoricalCrossentropy()
print("Categorical CE:", loss_cat(y_true_onehot, y_pred).numpy())

# Usando sparse_categorical_crossentropy (etiquetas como enteros)
loss_sparse = tf.keras.losses.SparseCategoricalCrossentropy()
print("Sparse Categorical CE:", loss_sparse(y_true_sparse, y_pred).numpy())





from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split

# Datos clasificación binaria
X, y = make_moons(n_samples=500, noise=0.2, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation="relu", input_shape=(2,)),
    tf.keras.layers.Dense(1, activation="sigmoid")
])

model.compile(optimizer="adam",
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=["accuracy"])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, verbose=0)

plt.plot(history.history["loss"], label="train")
plt.plot(history.history["val_loss"], label="val")
plt.legend()
plt.title("Binary Cross-Entropy en clasificación")
plt.show()


# Example multiclass: MNIST
(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
X_train = X_train.reshape(-1, 28*28).astype("float32") / 255.0
X_test = X_test.reshape(-1, 28*28).astype("float32") / 255.0

model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation="relu", input_shape=(784,)),
    tf.keras.layers.Dense(10, activation="softmax")
])

# Usamos sparse categorical, pues y son enteros [0..9]
model.compile(optimizer="adam",
              loss=tf.keras.losses.SparseCategoricalCrossentropy(),
              metrics=["accuracy"])

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)


pred = model.predict(X_test)
pred





model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation="relu", 
                          kernel_regularizer=tf.keras.regularizers.l2(1e-4),
                          input_shape=(784,)),
    tf.keras.layers.Dense(10, activation="softmax")
])









