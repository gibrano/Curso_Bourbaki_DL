{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75849539-99c4-40ac-b9ed-9921aae8fde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d0f4f6-4730-4004-86d6-55ca7ab21070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1628452-6c3f-4d1f-a72f-ec75a9cb65a5",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent & Optimization Algorithms\n",
    "\n",
    "We minimize a loss function $L(\\theta)$ using **gradient descent**:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla L(\\theta_t)\n",
    "$$\n",
    "\n",
    "Variants:\n",
    "- SGD (stochastic gradient descent)\n",
    "- Momentum\n",
    "- RMSProp\n",
    "- Adam\n",
    "\n",
    "Each changes the update dynamics depending on curvature of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c476ee05-c771-4668-8fa6-583da2112f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare optimizers on a simple regression\n",
    "X = np.linspace(-1, 1, 100).reshape(-1,1)\n",
    "y = X**3 + 0.1*np.random.randn(*X.shape)\n",
    "\n",
    "def build_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='tanh', input_shape=(1,)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "optimizers = {\n",
    "    \"SGD\": tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    \"Momentum\": tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),\n",
    "    \"RMSProp\": tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    \"Adam\": tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "for name, opt in optimizers.items():\n",
    "    model = build_model()\n",
    "    model.compile(optimizer=opt, loss='mse')\n",
    "    h = model.fit(X, y, epochs=100, verbose=0)\n",
    "    histories[name] = h.history['loss']\n",
    "\n",
    "# Plot\n",
    "for name, loss in histories.items():\n",
    "    plt.plot(loss, label=name)\n",
    "plt.legend()\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44e743a-0a71-41f2-a93d-dd7ac07f2484",
   "metadata": {},
   "source": [
    "## 4. Vanishing/Exploding Gradients\n",
    "\n",
    "During backpropagation, gradients involve products of Jacobians:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_0} =\n",
    "\\prod_{i=1}^n J_i \\cdot \\frac{\\partial L}{\\partial x_n}\n",
    "$$\n",
    "\n",
    "If eigenvalues of $J_i$ are < 1 â†’ **vanishing gradient**.  \n",
    "If > 1 â†’ **exploding gradient**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c54544-7393-4ca6-b7cf-936a395a3df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate vanishing gradient with deep sigmoid network\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "deep_model = tf.keras.Sequential(\n",
    "    [tf.keras.layers.Dense(32, activation='sigmoid', input_shape=(1,))] +\n",
    "    [tf.keras.layers.Dense(32, activation='sigmoid') for _ in range(10)] +\n",
    "    [tf.keras.layers.Dense(1)]\n",
    ")\n",
    "\n",
    "x_sample = tf.constant([[0.5]])\n",
    "with tf.GradientTape() as tape:\n",
    "    y_pred = deep_model(x_sample)\n",
    "grads = tape.gradient(y_pred, deep_model.trainable_variables)\n",
    "\n",
    "grad_norms = [tf.norm(g).numpy() for g in grads if g is not None]\n",
    "grad_norms[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a327964-e247-4985-bed0-643f875f8ff2",
   "metadata": {},
   "source": [
    "## 6. Initialization Schemes\n",
    "\n",
    "- **Xavier/Glorot** initialization: balances variance in forward/backward pass.  \n",
    "- **He initialization**: suited for ReLU-like activations.  \n",
    "\n",
    "This controls how activations and gradients propagate at initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8befa10a-c348-4a27-baba-d24668cde22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for init in [\"glorot_uniform\", \"he_normal\"]:\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=init, input_shape=(100,)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    x_rand = np.random.randn(1000, 100)\n",
    "    y_pred = model(x_rand)\n",
    "    print(init, \"output variance:\", np.var(y_pred.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39f03ff-265a-4d7e-ad25-88d0ece410de",
   "metadata": {},
   "source": [
    "## 7. Bridges: Linking Math & Deep Learning\n",
    "\n",
    "- **Activation choice â†’ gradient flow**  \n",
    "  Sigmoid/tanh cause vanishing gradients. ReLU/He init preserves signal.\n",
    "\n",
    "- **Optimization method â†’ curvature adaptation**  \n",
    "  SGD can get stuck in valleys; Momentum accelerates; RMSProp & Adam adapt to curvature.\n",
    "\n",
    "ðŸ‘‰ The combination of activation + initialization + optimizer determines\n",
    "how *learnable* a deep network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b504892-d78c-46f1-aee1-0670058e9c67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a2827-1f9c-46e7-9502-9bdf86bd7226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bdd103-58a4-429c-aa0f-e1f85311694d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
